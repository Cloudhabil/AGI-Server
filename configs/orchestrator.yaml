# Cognitive Orchestration Configuration
# Based on benchmark results from 2025-12-29
# Updated: 2026-01-13 (Substrate Equilibrium v0.5.0)

# =============================================================================
# SUBSTRATE EQUILIBRIUM MODE (v0.5.0)
# =============================================================================
# When enabled, enforces hardware sovereignty constraints:
# - VRAM cliff at 9750MB (prevents DWM contention)
# - NPU locked for all embedding tasks (frees PCIe bus)
# - System refuses to violate these physical laws
#
# Enable via: python manage.py local --substrate-equilibrium
# Or set environment: SUBSTRATE_EQUILIBRIUM=1
# =============================================================================

equilibrium:
  enabled: true  # HARDWARE SOVEREIGNTY - do not disable

  vram:
    cliff_mb: 9750           # Hard ceiling - DO NOT EXCEED
    cliff_pct: 81.25         # 9750/12000 = 81.25%
    dwm_reserve_mb: 1700     # Windows Desktop Window Manager
    safety_buffer_mb: 850    # Driver stability margin
    total_mb: 12288          # RTX 4070 SUPER

  npu:
    lock_embeddings: true    # Force all embeddings to NPU
    lock_vision: true        # Force vision preprocessing to NPU
    lock_audio: true         # Force audio preprocessing to NPU
    bypass_pcie: true        # NPU uses direct RAM path

  enforcement:
    refuse_violations: true  # System will refuse to exceed limits
    emergency_shutdown: true # Trigger shutdown on cliff breach
    log_breaches: true       # Log all limit breaches

compute:
  embeddings:
    # SUBSTRATE EQUILIBRIUM: NPU is now primary (frees PCIe for GPU)
    primary: npu   # Intel AI Boost - bypasses PCIe bus entirely
    fallback: cpu  # 1301 texts/sec - preserves VRAM
    # gpu: DISABLED for embeddings (conflicts with LLM inference)

  llm:
    device: gpu  # All models run on RTX 4070 SUPER via Ollama
    vram_limit_mb: 9750  # HARDWARE SOVEREIGNTY

models:
  codegemma:
    ollama_id: codegemma:latest
    role: fast
    speed: 133  # tok/s
    best_for:
      - quick_checks
      - formatting
      - simple_completions
      - intent_classification

  qwen3:
    ollama_id: qwen3:latest
    role: creative
    speed: 87  # tok/s
    best_for:
      - code_generation
      - idea_synthesis
      - creative_writing
      - solution_design

  deepseek-r1:
    ollama_id: deepseek-r1:latest
    role: analytical
    speed: 74  # tok/s
    best_for:
      - reasoning
      - analysis
      - debugging
      - critique
      - validation

routing:
  # Task type -> recommended model
  simple: codegemma
  code: qwen3
  reasoning: deepseek-r1
  analysis: deepseek-r1
  creative: qwen3
  debug: deepseek-r1
  default: qwen3

retrieval:
  engine: mshr  # Multi-Space Hierarchical Retrieval
  performance:
    pure_search: 80000  # queries/sec
    full_pipeline: 39  # queries/sec (with embedding)
    vs_sqlite: 17.8  # x faster than basic SQLite
  
  memory_types:
    episodic:
      description: Events and experiences
      recency_weight: 0.4
      threshold: 0.3
    semantic:
      description: Facts and knowledge
      importance_weight: 0.4
      threshold: 0.4
    procedural:
      description: Skills and methods
      frequency_weight: 0.3
      threshold: 0.5
    identity:
      description: Values and beliefs
      stability_bias: 0.6
      threshold: 0.6

hybrid_strategy:
  # When GPU is running LLM inference:
  background_tasks:
    - npu_embed_incoming_context
    - npu_precompute_likely_queries
    - cpu_memory_consolidation
  
  # Parallel processing:
  parallel:
    gpu: llm_inference
    npu: embedding_generation
    cpu: retrieval_scoring
