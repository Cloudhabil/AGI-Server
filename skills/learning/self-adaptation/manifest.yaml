id: learning/self-adaptation
name: Self-Adaptation
description: |
  Collect user corrections and preferences, train LoRA adapters in the background,
  and hot-swap them into the inference pipeline for personalized behavior.

  IMPORTANT: This is NOT instant. Training takes 5-30 minutes.
  The skill queues preferences and trains between sessions.

  Flow:
  1. User corrects GPIA response -> collect_preference
  2. Preferences accumulate in queue
  3. User triggers training -> train_adapter (background, 5-30 min)
  4. Adapter validated against benchmarks -> validate_adapter
  5. Adapter activated for inference -> activate_adapter
  6. New behavior persists across sessions

version: 1.0.0
level: advanced
category: learning

capabilities:
  - name: collect_preference
    description: Store a correction/preference pair for future training
    input:
      prompt: string (required) - The original prompt
      rejected_response: string (required) - The wrong/unwanted response
      preferred_response: string (required) - The correct/wanted response
      model: string - Which model generated the rejected response
      importance: float - Weight in training (default 1.0)
      domain: string - Domain for domain-specific adapters
    output:
      preference_id: string
      status: string

  - name: list_preferences
    description: View preferences queued for training
    input:
      domain: string (optional) - Filter by domain
      limit: integer - Max results (default 20)
    output:
      count: integer
      preferences: array

  - name: train_adapter
    description: Trigger background LoRA training (5-30 minutes)
    input:
      base_model: string - Base model to adapt (default qwen3:latest)
      domain: string (optional) - Train domain-specific adapter
      min_preferences: integer - Minimum preferences required (default 10)
      config: object - Training hyperparameters
    output:
      adapter_id: string
      status: string
      estimated_time: string

  - name: list_adapters
    description: View all trained adapters
    input:
      status: string (optional) - Filter by status
    output:
      adapters: array
      active_adapter: string

  - name: activate_adapter
    description: Load an adapter for inference
    input:
      adapter_id: string (required)
    output:
      model_name: string - Ollama model name to use
      status: string

  - name: validate_adapter
    description: Run benchmarks to check for regression
    input:
      adapter_id: string (required)
    output:
      validation_score: float
      passed: boolean

  - name: status
    description: Get current self-adaptation status
    output:
      preferences_queued: integer
      adapters_ready: integer
      can_train: boolean

requirements:
  python:
    - torch>=2.0
    - transformers>=4.36
    - peft>=0.7
    - datasets>=2.14
    - bitsandbytes>=0.41
    - trl>=0.7

hardware:
  min_vram_gb: 8
  recommended_vram_gb: 12
  supports_cpu: false
  notes: |
    Training uses QLoRA (4-bit quantization) to fit in 12GB VRAM.
    RTX 4070 SUPER can train 7B models in 5-30 minutes.

limitations:
  - NOT real-time: Training takes 5-30 minutes
  - Requires GPU: CPU training is impractically slow
  - Min 10 preferences: Need enough data for meaningful adaptation
  - No live injection: Adapter swap requires model reload
  - Ollama constraints: Requires pre-merging adapters into Modelfile

author: GPIA
tags:
  - learning
  - fine-tuning
  - lora
  - personalization
  - preference-learning
